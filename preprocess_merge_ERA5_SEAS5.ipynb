{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.08321,
     "end_time": "2022-03-14T17:49:44.888825",
     "exception": false,
     "start_time": "2022-03-14T17:49:44.805615",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "![logo](./img/LogoLine_horizon_C3S.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.081751,
     "end_time": "2022-03-14T17:49:45.053733",
     "exception": false,
     "start_time": "2022-03-14T17:49:44.971982",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.082814,
     "end_time": "2022-03-14T17:49:45.21891",
     "exception": false,
     "start_time": "2022-03-14T17:49:45.136096",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Merge Reanalysys (ERA5) and Hindcast (SEAS5) for modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.080985,
     "end_time": "2022-03-14T17:49:45.381362",
     "exception": false,
     "start_time": "2022-03-14T17:49:45.300377",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### About"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.080595,
     "end_time": "2022-03-14T17:49:45.706112",
     "exception": false,
     "start_time": "2022-03-14T17:49:45.625517",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "\n",
    "The notebook has the following outline:\n",
    "* 1 - Join hindcast files: tp and t2m for each month, year and member\n",
    "* 2 - Merge ERA5 with hindcast files (ERA5: warm up period=5 years)\n",
    "* 3 - Combine datasets of subcatchments (C1 and C2) for modelling (PERSiST)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.081079,
     "end_time": "2022-03-14T17:49:46.683509",
     "exception": false,
     "start_time": "2022-03-14T17:49:46.60243",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Install packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "papermill": {
     "duration": 12.174934,
     "end_time": "2022-03-14T17:49:58.941932",
     "exception": false,
     "start_time": "2022-03-14T17:49:46.766998",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: cdsapi in c:\\users\\apedregal\\anaconda3\\lib\\site-packages (0.6.1)\n",
      "Requirement already satisfied: requests>=2.5.0 in c:\\users\\apedregal\\anaconda3\\lib\\site-packages (from cdsapi) (2.26.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\apedregal\\anaconda3\\lib\\site-packages (from cdsapi) (4.62.3)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\apedregal\\anaconda3\\lib\\site-packages (from requests>=2.5.0->cdsapi) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\apedregal\\anaconda3\\lib\\site-packages (from requests>=2.5.0->cdsapi) (3.2)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\apedregal\\anaconda3\\lib\\site-packages (from requests>=2.5.0->cdsapi) (1.26.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\apedregal\\anaconda3\\lib\\site-packages (from requests>=2.5.0->cdsapi) (2024.2.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\apedregal\\anaconda3\\lib\\site-packages (from tqdm->cdsapi) (0.4.4)\n"
     ]
    }
   ],
   "source": [
    "# Install CDS API for downloading data from the CDS\n",
    "#!pip install cdsapi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 127.036612,
     "end_time": "2022-03-14T17:52:06.036378",
     "exception": false,
     "start_time": "2022-03-14T17:49:58.999766",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Install cfgrib to enable us to read GRIB format files\n",
    "#!conda install -c conda-forge cfgrib -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "papermill": {
     "duration": 2.519057,
     "end_time": "2022-03-14T17:52:09.394032",
     "exception": false,
     "start_time": "2022-03-14T17:52:06.874975",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Miscellaneous operating system interfaces\n",
    "import os\n",
    "\n",
    "# CDS API\n",
    "import cdsapi\n",
    "\n",
    "# To map GRIB files to NetCDF Common Data Model\n",
    "import cfgrib\n",
    "\n",
    "# Libraries for working with multi-dimensional arrays\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import scipy\n",
    "\n",
    "# Libraries for plotting and geospatial data visualisation\n",
    "import matplotlib.path as mpath\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import cartopy.crs as ccrs\n",
    "from cartopy.mpl.gridliner import LONGITUDE_FORMATTER, LATITUDE_FORMATTER\n",
    "import cartopy.feature as cfeature\n",
    "\n",
    "# To work with data labels in dictionary format\n",
    "from collections import OrderedDict\n",
    "\n",
    "# Date and time related libraries\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from calendar import monthrange\n",
    "import datetime\n",
    "\n",
    "# Interactive HTML widgets\n",
    "import ipywidgets as widgets\n",
    "\n",
    "# Disable warnings for data download via API\n",
    "import urllib3 \n",
    "urllib3.disable_warnings()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we specify a data directory where the hindcast files are located and where we will save generated files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = 'C:/Users/apedregal/Documents/inventWater_docs/Modelling/Seasonal forecasts/seasonal/hindcast'\n",
    "output_path = 'C:/Users/apedregal/Documents/inventWater_docs/Modelling/Seasonal forecasts/seasonal/hindcast_joined'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.274576,
     "end_time": "2022-03-14T17:52:10.724408",
     "exception": false,
     "start_time": "2022-03-14T17:52:10.449832",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 1. Join precipitation and temperature hindcast files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate temperature filenames\n",
    "def generate_temperature_filename(member, year, month):\n",
    "    return f\"hind_C2_t2m_member{member}_year{year}_month{month}.csv\"\n",
    "\n",
    "# Function to generate precipitation filenames\n",
    "def generate_precipitation_filename(member, time):\n",
    "    return f\"hind_C2_tp_member{member}_time{time}.csv\"\n",
    "\n",
    "# Time indices for precipitation files (0 to 287)\n",
    "precipitation_times = range(288)\n",
    "\n",
    "# Loop through each member\n",
    "for member in range(25):\n",
    "    # Counters to track the current year and month for temperature files\n",
    "    current_year = 0\n",
    "    current_month = 0\n",
    "    \n",
    "    for time in precipitation_times:\n",
    "        # Generate filenames\n",
    "        temperature_filename = generate_temperature_filename(member, current_year, current_month)\n",
    "        precipitation_filename = generate_precipitation_filename(member, time)\n",
    "        \n",
    "        # Construct full file paths\n",
    "        temperature_filepath = os.path.join(input_path, temperature_filename)\n",
    "        precipitation_filepath = os.path.join(input_path, precipitation_filename)\n",
    "        \n",
    "        # Check if files exist before processing\n",
    "        if not os.path.exists(temperature_filepath) or not os.path.exists(precipitation_filepath):\n",
    "            print(f\"Skipping non-existing file pair: {temperature_filename} and {precipitation_filename}\")\n",
    "            continue\n",
    "        \n",
    "        # Read the temperature and precipitation CSV files\n",
    "        temperature_df = pd.read_csv(temperature_filepath, header=None, names=['Temperature'])\n",
    "        precipitation_df = pd.read_csv(precipitation_filepath, header=None, names=['Precipitation'])\n",
    "        \n",
    "        # Merge the data\n",
    "        merged_df = pd.concat([precipitation_df, temperature_df], axis=1)\n",
    "        \n",
    "        # Save the merged data to a new CSV file\n",
    "        output_filename = f\"hind_C2_joined_member{member}_year{current_year}_month{current_month}.csv\"  \n",
    "        merged_df.to_csv(os.path.join(output_path, output_filename), sep=' ', index=False)\n",
    "        \n",
    "        # Update the year and month counters\n",
    "        current_month += 1\n",
    "        if current_month > 11:\n",
    "            current_month = 0\n",
    "            current_year += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 - Merge ERA5 with hindcast files (ERA5: warm up period=5 years)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set path directories\n",
    "path_reanalysis = 'C:/Users/apedregal/Documents/inventWater_docs/Modelling/Seasonal forecasts/reanalysis'\n",
    "path_hindcast = 'C:/Users/apedregal/Documents/inventWater_docs/Modelling/Seasonal forecasts/seasonal/hindcast_joined'\n",
    "path_merged = 'C:/Users/apedregal/Documents/inventWater_docs/Modelling/Seasonal forecasts/seasonal/hindcast_merged'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TEST TO MERGE ERA5 AND HINDCAST\n",
    "\n",
    "# Load reanalysis data\n",
    "df = pd.read_csv(f'{path_reanalysis}/reanalysis_daily_C1_all_withDates.csv')\n",
    "\n",
    "# Split 'time tp t2m' column into separate columns\n",
    "df[['time', 'tp', 't2m']] = df['time tp t2m'].str.split(expand=True)\n",
    "\n",
    "# Drop the original 'time tp t2m' column\n",
    "df.drop(columns=['time tp t2m'], inplace=True)\n",
    "\n",
    "# Define the start and end dates for the 5-year period\n",
    "start_date = '2019-01-01'\n",
    "end_date = '2024-01-01'\n",
    "\n",
    "# Filter the DataFrame based on the specified date range\n",
    "df_5_years = df[(df['time'] >= start_date) & (df['time'] < end_date)]\n",
    "\n",
    "# Select meteo data only without dates\n",
    "df_era5 = df_5_years[['tp', 't2m']]\n",
    "\n",
    "# Load hindcast data\n",
    "df_hind = pd.read_csv(f'{path_hindcast}/hind_C1_joined_member0_year0_month0.csv')\n",
    "\n",
    "# Split 'Precipitation Temperature' column into separate columns\n",
    "df_hind[['tp', 't2m']] = df_hind['Precipitation Temperature'].str.split(expand=True)\n",
    "\n",
    "# Drop the original 'Precipitation Temperature' column\n",
    "df_hind.drop(columns=['Precipitation Temperature'], inplace=True)\n",
    "\n",
    "# Merge reanalysis and hindcast data\n",
    "df_merged = pd.concat([df_era5, df_hind], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tp</th>\n",
       "      <th>t2m</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>3.1316223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>3.1357422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.1793518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.42651367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2.979004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2036</th>\n",
       "      <td>0.0</td>\n",
       "      <td>20.076019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2037</th>\n",
       "      <td>0.0</td>\n",
       "      <td>19.072723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2038</th>\n",
       "      <td>0.0</td>\n",
       "      <td>19.254303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2039</th>\n",
       "      <td>0.0</td>\n",
       "      <td>19.23053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2040</th>\n",
       "      <td>0.1220703125</td>\n",
       "      <td>19.071747</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2041 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                tp         t2m\n",
       "0              0.0   3.1316223\n",
       "1              0.0   3.1357422\n",
       "2              0.0   1.1793518\n",
       "3              0.0  0.42651367\n",
       "4              0.0    2.979004\n",
       "...            ...         ...\n",
       "2036           0.0   20.076019\n",
       "2037           0.0   19.072723\n",
       "2038           0.0   19.254303\n",
       "2039           0.0    19.23053\n",
       "2040  0.1220703125   19.071747\n",
       "\n",
       "[2041 rows x 2 columns]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TEST TO EXTRACT DATE FROM HINDCAST FILES\n",
    "\n",
    "import glob\n",
    "\n",
    "def extract_date(year_numeric, month_numeric):\n",
    "    # Map numeric year to actual year\n",
    "    year = 1993 + year_numeric\n",
    "    \n",
    "    # Map numeric month to actual month\n",
    "    month_mapping = {\n",
    "        0: 'January', 1: 'February', 2: 'March', 3: 'April',\n",
    "        4: 'May', 5: 'June', 6: 'July', 7: 'August',\n",
    "        8: 'September', 9: 'October', 10: 'November', 11: 'December'\n",
    "    }\n",
    "    month = month_mapping[month_numeric]\n",
    "    \n",
    "    # Convert year and month to date without time\n",
    "    date = pd.to_datetime(str(year) + '-' + str(month_numeric + 1), format='%Y-%m').date()\n",
    "    \n",
    "    return date\n",
    "\n",
    "# Find all CSV files matching the pattern\n",
    "csv_files = glob.glob(path_hindcast + '/*.csv')\n",
    "\n",
    "for csv_file in csv_files:\n",
    "    # Extract year and month from file name\n",
    "    file_parts = csv_file.split('_')\n",
    "    member = int(file_parts[-3][6:])  # Extract member number from the third to last part\n",
    "    year_numeric = int(file_parts[-2][4:])  # Extract numeric year from the second to last part\n",
    "    month_numeric = int(file_parts[-1].split('.')[0][5:])  # Extract numeric month from the last part\n",
    "    \n",
    "    # Extract date using the function\n",
    "    date = extract_date(year_numeric, month_numeric)\n",
    "    \n",
    "    # Display the details\n",
    "    #print(f\"CSV File: {csv_file}, Member: {member}, Date: {date}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final code to merge datasets and create csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "reach='C2' #Subcatchments, reach1=C1, reach2=C2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing complete.\n"
     ]
    }
   ],
   "source": [
    "def extract_date(year_numeric, month_numeric):\n",
    "    # Map numeric year to actual year\n",
    "    year = 1993 + year_numeric\n",
    "    \n",
    "    # Convert to date object\n",
    "    return pd.to_datetime(f'{year}-{month_numeric + 1}', format='%Y-%m').date()\n",
    "\n",
    "def get_reanalysis_data_for_period(reanalysis_df, end_date):\n",
    "    # Convert end_date to datetime\n",
    "    end_date = pd.to_datetime(end_date)\n",
    "    start_date = end_date - pd.DateOffset(years=5)\n",
    "    \n",
    "    # Filter the reanalysis DataFrame based on the specified date range\n",
    "    df_5_years = reanalysis_df[(reanalysis_df['time'] >= start_date.strftime('%Y-%m-%d')) & \n",
    "                               (reanalysis_df['time'] < end_date.strftime('%Y-%m-%d'))]\n",
    "    \n",
    "    # Select meteo data only without dates\n",
    "    df_era5 = df_5_years[['tp', 't2m']]\n",
    "    return df_era5\n",
    "\n",
    "# Load reanalysis data\n",
    "df_reanalysis = pd.read_csv(f'{path_reanalysis}/reanalysis_daily_{reach}_all_withDates.csv')\n",
    "\n",
    "# Split 'time tp t2m' column into separate columns\n",
    "df_reanalysis[['time', 'tp', 't2m']] = df_reanalysis['time tp t2m'].str.split(expand=True)\n",
    "\n",
    "# Drop the original 'time tp t2m' column\n",
    "df_reanalysis.drop(columns=['time tp t2m'], inplace=True)\n",
    "\n",
    "# Specify the ranges for member, year, and month\n",
    "members = range(25)  # 0 to 24\n",
    "years = range(24)  # 0 to 23\n",
    "months = range(12)  # 0 to 11\n",
    "\n",
    "for member in members:\n",
    "    for year in years:\n",
    "        for month in months:\n",
    "            # Construct the filename\n",
    "            csv_file = f\"{path_hindcast}/hind_{reach}_joined_member{member}_year{year}_month{month}.csv\"\n",
    "            \n",
    "            # Check if the file exists\n",
    "            if os.path.exists(csv_file):\n",
    "                # Extract date using the function\n",
    "                date = extract_date(year, month)\n",
    "                \n",
    "                # Get reanalysis data for the 5 years before the extracted date\n",
    "                df_era5 = get_reanalysis_data_for_period(df_reanalysis, date)\n",
    "                \n",
    "                # Load hindcast data\n",
    "                df_hind = pd.read_csv(csv_file)\n",
    "                \n",
    "                # Split 'Precipitation Temperature' column into separate columns\n",
    "                df_hind[['tp', 't2m']] = df_hind['Precipitation Temperature'].str.split(expand=True)\n",
    "                \n",
    "                # Drop the original 'Precipitation Temperature' column\n",
    "                df_hind.drop(columns=['Precipitation Temperature'], inplace=True)\n",
    "                \n",
    "                # Merge reanalysis and hindcast data\n",
    "                df_merged = pd.concat([df_era5, df_hind], ignore_index=True)\n",
    "                \n",
    "                # Save the merged DataFrame to a new CSV file with a space separator\n",
    "                output_file = os.path.join(path_merged, f\"hind_{reach}_merged_member{member}_year{year}_month{month}_date{date}.csv\")\n",
    "                df_merged.to_csv(output_file, index=False)\n",
    "                \n",
    "print(\"Processing complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 - Combine datasets of subcatchments (C1 and C2) for modelling (PERSiST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = 'C:/Users/apedregal/Documents/inventWater_docs/Modelling/Seasonal forecasts/seasonal/hindcast_merged'\n",
    "output_path = 'C:/Users/apedregal/Documents/inventWater_docs/Modelling/Seasonal forecasts/seasonal/hindcast_modelling'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "# Find all CSV files in the input directory\n",
    "csv_files = glob.glob(os.path.join(input_path, '*.csv'))\n",
    "\n",
    "# Create a dictionary to hold pairs of files\n",
    "file_pairs = {}\n",
    "\n",
    "# Identify and pair up files with 'C1' and 'C2' in the filename\n",
    "for file in csv_files:\n",
    "    filename = os.path.basename(file)\n",
    "    if 'C1' in filename:\n",
    "        key = filename.replace('C1', '')\n",
    "        if key not in file_pairs:\n",
    "            file_pairs[key] = {}\n",
    "        file_pairs[key]['C1'] = file\n",
    "    elif 'C2' in filename:\n",
    "        key = filename.replace('C2', '')\n",
    "        if key not in file_pairs:\n",
    "            file_pairs[key] = {}\n",
    "        file_pairs[key]['C2'] = file\n",
    "\n",
    "# Process each pair of files\n",
    "for key, pair in file_pairs.items():\n",
    "    if 'C1' in pair and 'C2' in pair:\n",
    "        # Load the CSV files into pandas DataFrames\n",
    "        df1 = pd.read_csv(pair['C1'])\n",
    "        df2 = pd.read_csv(pair['C2'])\n",
    "\n",
    "        # Get the number of rows in df1\n",
    "        num_rows_df1 = len(df1)\n",
    "\n",
    "        # Create new rows without decimals\n",
    "        new_row_df1 = pd.DataFrame({'tp': [str(num_rows_df1)], 't2m': [None]})\n",
    "        new_row_2 = pd.DataFrame({'tp': [str(2)], 't2m': [None]})\n",
    "        new_row_3 = pd.DataFrame({'tp': ['C1'], 't2m': [None]})\n",
    "\n",
    "        # Add the rows in the correct order to df1\n",
    "        df1 = pd.concat([new_row_df1, new_row_2, new_row_3, df1]).reset_index(drop=True)\n",
    "\n",
    "        # Add the name 'C2' as a new row at index 0 in df2\n",
    "        new_row_4 = pd.DataFrame({'tp': ['C2'], 't2m': [None]})\n",
    "        df2 = pd.concat([new_row_4, df2], ignore_index=True)\n",
    "\n",
    "        # Concatenate the DataFrames\n",
    "        result = pd.concat([df1, df2], ignore_index=True)\n",
    "\n",
    "        # Write the result to a new CSV file without column headers and with space separator\n",
    "        output_filename = f'{key}.csv'\n",
    "        result.to_csv(os.path.join(output_path, output_filename), index=False, header=False, sep=' ')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
